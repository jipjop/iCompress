{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iCompress - Compression based on eye tracking behavior\n",
    "\n",
    "iCompress features image region-based compression made possible by extracting eye fixation regions from fixation data. <br />\n",
    "iCompress transforms the eye fixation data to Fixation Density Maps (FDMs) and utilizes them to determine the image regions that should and should not be compressed. <br />\n",
    "Encoding and decoding of an image is based on the JPEG compression algorithm.\n",
    "More specifically, iCompress goes through similar steps to encode an image, but differs from it by using weighted quantization tables with respect to the degree of fixation found in the FDMs. <br />\n",
    "Furthermore, iCompress leaves regions-of-interest (i.e. high degree of fixation) in tact and does not compress them. <br />\n",
    "iCompress saves its files in <code>.icy</code> format.\n",
    "\n",
    "This python notebook demonstrates the application of the algorithm on both images and videos with fixation data.\n",
    "Throughout this notebook several functions (with possible small modifcations) from the CS4065 Multimedia Search and Recommendation Course Systems Lab 1 were used and accredited in line as such.\n",
    "\n",
    "**To get started, please ensure that all the dependencies are installed!** <br />\n",
    "\n",
    "## Dependencies:\n",
    "* matplot-lib: https://matplotlib.org/users/installing.html\n",
    "* OpenCV2: https://gist.github.com/arthurbeggs/06df46af94af7f261513934e56103b30\n",
    "* scikit-image: http://scikit-image.org/download\n",
    "\n",
    "## References:\n",
    "* [1] H. Nemoto, P. Hanhart, P. Korshunov, and T. Ebrahimi, \"Ultra-Eye: UHD and HD images eye tracking dataset.\" In International Workshop on Quality of Multimedia Experience (QoMEX), September 2014\n",
    "* [2] T. Vigier, J. Rousseau, M. Perreira Da Silva, and P. Le Callet, “A new HD and UHD video eye tracking dataset.” In 7th ACM Multimedia Systems Conference (MMSys), May 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports\n",
    "We first require to import all the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import copy\n",
    "import csv\n",
    "from Queue import PriorityQueue\n",
    "import subprocess as sp\n",
    "import struct\n",
    "import collections\n",
    "import skimage.measure\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions\n",
    "These helper functions were taken from [cvtools.py](https://raw.githubusercontent.com/mmc-tudelft/cs4065/master/cvtools.py) from the MMR repository and slightly modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_numpy_image_size(image_size):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    height, width, channels = parse_numpy_image_size(np.shape(image))\n",
    "    \"\"\"\n",
    "    try:\n",
    "        height, width, channels = image_size\n",
    "    except:\n",
    "        height, width = image_size\n",
    "        channels = 1\n",
    "    return height, width, channels\n",
    "\n",
    "def format_image_size(image_size):\n",
    "    \"\"\"\n",
    "    Usage:\n",
    "    formatted_image_size = format_image_size(np.shape(image))\n",
    "    \"\"\"\n",
    "    height, width, channels = parse_numpy_image_size(image_size)\n",
    "    return '%d x %d (%d channels)' % (width, height, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_DEFAULT_IMAGE_FIGSIZE = (16, 16)\n",
    "IMAGE_HEIGHT = 2160\n",
    "IMAGE_WIDTH = 3840\n",
    "\n",
    "def ipynb_show_cv2_image(image_in, title='', figsize=_DEFAULT_IMAGE_FIGSIZE, grayscale = False, save=False):\n",
    "    _, _, channels = parse_numpy_image_size(np.shape(image_in))\n",
    "    image = image_in.copy()\n",
    "    if channels > 1:\n",
    "        image[:, :, 0] = image_in[:, :, 2]\n",
    "        image[:, :, 2] = image_in[:, :, 0]\n",
    "    ipynb_show_image(image, title, figsize, grayscale, save)\n",
    "\n",
    "def ipynb_show_image(image, title='', figsize=_DEFAULT_IMAGE_FIGSIZE, grayscale = False, save=False):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_title(title)\n",
    "    if grayscale:\n",
    "        plt.imshow(image, cmap = 'gray')\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(title + '.png')\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in one of the images from the UltraEye dataset [1]\n",
    "To test the algorithm for combining the fixation density maps (FDMs) with the original image and splitting it into macroblocks,\n",
    "we can take an image from our UHD image dataset and its FDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First tryout of reading image\n",
    "im = cv2.imread(\"ultraeye/C01_UHD.tif\", cv2.IMREAD_UNCHANGED)\n",
    "im_size = np.shape(im)\n",
    "\n",
    "print 'Original image size: %s' % format_image_size(im_size)\n",
    "ipynb_show_cv2_image(im, title = 'Source image: C01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set size for our macro block\n",
    "MACROBLOCK_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot one of our fixation density maps\n",
    "im_fdm = cv2.imread(\"ultraeye/C01_FDM_UHD.tif\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "im_size_fdm = np.shape(im_fdm)\n",
    "print 'Original image size: %s' % format_image_size(im_size_fdm)\n",
    "\n",
    "# Plot our fixation density map\n",
    "ipynb_show_cv2_image(im_fdm, title = 'Fixation density map: C01', grayscale = True)\n",
    "\n",
    "# Pre-process our FDM for selecting the more relevant sections\n",
    "# May not be necessary for selecting the macroblocks in the end, but is nice for the visualization.\n",
    "MAX_PIX_VAL = 255\n",
    "THRESHOLD_SCALAR = 0.2\n",
    "white_threshold = THRESHOLD_SCALAR*MAX_PIX_VAL\n",
    "_, im_fdm_thresh = cv2.threshold(im_fdm, white_threshold, MAX_PIX_VAL, cv2.THRESH_BINARY)\n",
    "ipynb_show_cv2_image(im_fdm_thresh, title = 'Thresholded fixation density map: C01', grayscale = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the combination of the thresholded FDM and original image.\n",
    "From our previous (thresholded) FDM, we now know what regions are 'important'. These regions are available to us as pixel coordinates. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the pixel coordinates of a FDM\n",
    "def computeFDMRegionCoordinates(fdm):\n",
    "    coordinates = []\n",
    "    for i in range(0, len(fdm)):\n",
    "        coords = (fdm[i].flatten()[0], fdm[i].flatten()[1])\n",
    "        coordinates.append(coords)\n",
    "    return coordinates\n",
    "\n",
    "# Draw the FDM region on top of an image.\n",
    "# Input:\n",
    "# - Image to draw FDM on top of\n",
    "# - Coordinates of the FDM to draw\n",
    "def drawFDMRegions(image, coordinates):\n",
    "    for i in range(0, len(coordinates)):\n",
    "        coords = (coordinates[i][0], coordinates[i][1])\n",
    "        cv2.circle(image, coords, 1, (0, 0, 255), -1, cv2.CV_AA)\n",
    "\n",
    "    ipynb_show_cv2_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our FDM looks like on top of our original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonZero_fdm = cv2.findNonZero(im_fdm_thresh)\n",
    "if nonZero_fdm is not None:\n",
    "    region_fdm_image = cv2.imread(\"ultraeye/C01_UHD.tif\", cv2.IMREAD_UNCHANGED)\n",
    "    num_rows = nonZero_fdm.shape[0]/im_size_fdm[1]\n",
    "\n",
    "    im_FDMcoords = computeFDMRegionCoordinates(nonZero_fdm)\n",
    "    drawFDMRegions(region_fdm_image, im_FDMcoords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's left is to take the inverse and use those pixel coordinates and map them to the macro block coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the inverted regions, so we can use the indices as selectors for our region based compression\n",
    "inverted_region_fdm_image = cv2.imread(\"ultraeye/C01_UHD.tif\", cv2.IMREAD_UNCHANGED)\n",
    "inverted_fdm_thres = cv2.bitwise_not(im_fdm_thresh)\n",
    "inverted_fdm = cv2.bitwise_not(im_fdm)\n",
    "\n",
    "# Show the inverted FDM\n",
    "ipynb_show_cv2_image(inverted_fdm, title = 'Inverted fixation density map: C01', grayscale = True)\n",
    "\n",
    "# Show the inverted thresholded FDM\n",
    "ipynb_show_cv2_image(inverted_fdm_thres, title = 'Inverted thresholded fixation density map: C01', grayscale = True)\n",
    "\n",
    "# This code shows the inverted FDM on top of our original, but is rather slow and not needed for the actual compression\n",
    "# nonZero_inverted_fdm = cv2.findNonZero(inverted_fdm_thres)\n",
    "\n",
    "# inverted_im_FDMcoords = computeFDMRegionCoordinates(nonZero_inverted_fdm)\n",
    "# # Draw the image with our inverted FDM regions once again\n",
    "# drawFDMRegions(inverted_region_fdm_image, inverted_im_FDMcoords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions taken from the MMSR systems lab 1.\n",
    "These are some functions to determine the macroblock coordinates given pixel coordinates and converting back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_macroblock_coords((x, y), block_size = MACROBLOCK_SIZE):\n",
    "    \"\"\"Computes the macroblock coordinates of a pixel given the (square) block size.\"\"\"\n",
    "    def _macroblock_index(pos):\n",
    "    # This is the core function that does the job for get_number_of_macroblocks().\n",
    "        return pos/block_size\n",
    "    return (_macroblock_index(x), _macroblock_index(y))\n",
    "\n",
    "def get_number_of_macroblocks(image_size, block_size = MACROBLOCK_SIZE):\n",
    "    \"\"\"Returns the number of horizontal and vertical macroblocks.\"\"\"\n",
    "    return get_macroblock_coords((image_size[1], image_size[0]), block_size)\n",
    "\n",
    "def macroblock_coords_to_pixel_vertices(macroblock_coords, block_size = MACROBLOCK_SIZE):\n",
    "    \"\"\"Maps a macroblock onto pixel vertex coordinates.\"\"\"\n",
    "    x0 = macroblock_coords[0]*block_size\n",
    "    y0 = macroblock_coords[1]*block_size\n",
    "    x1 = macroblock_coords[0]*block_size + block_size - 1\n",
    "    y1 = macroblock_coords[1]*block_size + block_size - 1\n",
    "    return (x0, y0), (x1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate macroblocks in our image\n",
    "Taken from the taken from the MMSR systems lab 1, we inspect our image's macroblocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Image size: %s' % format_image_size(im_size)\n",
    "print 'Number of macroblocks: %d horizontal and %d vertical' % get_number_of_macroblocks(im_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1.\n",
    "# Parameters (explained below when used for the first time).\n",
    "USE_ANTIALIASING_FILTER = True\n",
    "FILTER_SIZE = (2, 2)\n",
    "SUBSAMPLING_FACTORS = (2, 2)  # Vertical and horizontal. https://en.wikipedia.org/wiki/Chroma_subsampling\n",
    "# These numbers mean that we skip X-1 vertical pixels in original image (step size), and skip Y-1 horizontal.\n",
    "# So (1,2) means step size 1 for vertical, taking all pixels into account and 2 means halving horizontal pixel.\n",
    "# We get therefore a 4:2:2 subsampling.\n",
    "# For (2,2), we get 4:2:0 subsampling.\n",
    "# (1,1) is 4:4:4 subsampling.\n",
    "# (1,4) is 4:1:1\n",
    "# (4,2) is 4:4:0\n",
    "\n",
    "def subsample_chrominance_channels(image):\n",
    "    # Let's convert the RGB image to the YUV space.\n",
    "    # You will apply compression in such color space.\n",
    "    image_YCrCb = cv2.cvtColor(image, cv2.cv.CV_BGR2YCrCb)\n",
    "\n",
    "    # Anti-aliasing filter.\n",
    "    # Before we subsample the Cr and Cb channels, we apply an averaging filter to avoid artifacts.\n",
    "    # We use cv2.boxFilter() for this (see http://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#boxfilter).\n",
    "    if USE_ANTIALIASING_FILTER:\n",
    "        image_Cr_channel_filtered = cv2.boxFilter(image_YCrCb[:,:,1], ddepth=-1, ksize=FILTER_SIZE)\n",
    "        image_Cb_channel_filtered = cv2.boxFilter(image_YCrCb[:,:,2], ddepth=-1, ksize=FILTER_SIZE)\n",
    "    else:\n",
    "        image_Cr_channel_filtered = image_YCrCb[:,:,1]\n",
    "        image_Cb_channel_filtered = image_YCrCb[:,:,2]\n",
    "\n",
    "    # Subsampling step: we will skip crominance values (similar to image resizing).\n",
    "    # We use numpy array slicing (see http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html).\n",
    "    image_Cr_channel_subsampled = image_Cr_channel_filtered[\n",
    "      ::SUBSAMPLING_FACTORS[0], ::SUBSAMPLING_FACTORS[1]]\n",
    "    image_Cb_channel_subsampled = image_Cb_channel_filtered[\n",
    "      ::SUBSAMPLING_FACTORS[0], ::SUBSAMPLING_FACTORS[1]]\n",
    "\n",
    "    return [\n",
    "        image_YCrCb[:,:,0],  # We don't toch the luminance channel.\n",
    "        image_Cr_channel_subsampled,\n",
    "        image_Cb_channel_subsampled\n",
    "    ]\n",
    "\n",
    "image_YCrCb_subsampled = subsample_chrominance_channels(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining regions of our image to compress\n",
    "To compress only the regions with respect to the FDM, we first divide our image into macroblocks. <br />\n",
    "Then we create a boolean matrix indicating which macroblocks should be quantized later on. <br />\n",
    "We give a 1, indicating that the macroblock should be quantized, iff the entire macroblock is found inside the coordinates.\n",
    "Otherwise, when a single pixel coordinate inside a macroblock is not found, the macroblock should be left untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print our channel sizes\n",
    "print 'Y size: ' + str(image_YCrCb_subsampled[0].shape)\n",
    "print 'Cr size: ' + str(image_YCrCb_subsampled[1].shape)\n",
    "print 'Cb size: ' + str(image_YCrCb_subsampled[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take in a channel and convert that channel into macroblocks\n",
    "def computeSelectionMacroBlock(channel):\n",
    "    channel_FDMsize = channel.shape\n",
    "\n",
    "    num_macroblock = get_number_of_macroblocks(channel_FDMsize)\n",
    "    print 'Shape macro block: ' + str(num_macroblock)\n",
    "    macroblock_im_FDMcoords = np.zeros((num_macroblock[0], num_macroblock[1]))\n",
    "\n",
    "    for i in range(0, channel_FDMsize[0], MACROBLOCK_SIZE):\n",
    "        for j in range(0, channel_FDMsize[1], MACROBLOCK_SIZE):\n",
    "            macroblock_im_FDMcoords[j/MACROBLOCK_SIZE, i/MACROBLOCK_SIZE] = np.amin(channel[i:i+MACROBLOCK_SIZE-1, j:j+MACROBLOCK_SIZE-1])\n",
    "    return macroblock_im_FDMcoords\n",
    "\n",
    "# Takes in a FDM and computes the regions which we need\n",
    "def computeChannelRegions(FDM):\n",
    "    # Leave Y alone\n",
    "    Y_bool_inverted_regions = FDM.astype(bool)*1 # Make a boolean 0 and 1 matrix\n",
    "\n",
    "    # Use subsampled boolean FDM for Cr and Cb\n",
    "    Cr_bool_inverted_regions = Y_bool_inverted_regions[::SUBSAMPLING_FACTORS[0], ::SUBSAMPLING_FACTORS[1]]\n",
    "    Cb_bool_inverted_regions = Y_bool_inverted_regions[::SUBSAMPLING_FACTORS[0], ::SUBSAMPLING_FACTORS[1]]\n",
    "\n",
    "    Y_macro_blocks = computeSelectionMacroBlock(Y_bool_inverted_regions)\n",
    "    Cr_macro_blocks = computeSelectionMacroBlock(Cr_bool_inverted_regions)\n",
    "    Cb_macro_blocks = computeSelectionMacroBlock(Cb_bool_inverted_regions)\n",
    "    return [Y_macro_blocks, Cr_macro_blocks, Cb_macro_blocks]\n",
    "\n",
    "channelRegions = computeChannelRegions(inverted_fdm_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1\n",
    "# Luminance and chrominance quantization tables.\n",
    "# (from Annex K of https://www.w3.org/Graphics/JPEG/itu-t81.pdf).\n",
    "def parse_quantization_table(data):\n",
    "    return np.array([[\n",
    "      np.float64(cell) for cell in row.split(' ')] for row in (\n",
    "          data.split('\\n'))], dtype=np.float64)\n",
    "\n",
    "# Quantization table to be used on the Y channel.\n",
    "JPEG_STANDARD_LUMINANCE_QUANTIZATION_TABLE = parse_quantization_table(\n",
    "\"\"\"16 11 10 16 24 40 51 61\n",
    "12 12 14 19 26 58 60 55\n",
    "14 13 16 24 40 57 69 56\n",
    "14 17 22 29 51 87 80 62\n",
    "18 22 37 56 68 109 103 77\n",
    "24 35 55 64 81 104 113 92\n",
    "49 64 78 87 103 121 120 101\n",
    "72 92 95 98 112 100 103 99\"\"\")\n",
    "\n",
    "# Quantization table to be used on the Cr and Cb channels.\n",
    "JPEG_STANDARD_CHROMINANCE_QUANTIZATION_TABLE = parse_quantization_table(\n",
    "\"\"\"17 18 24 47 99 99 99 99\n",
    "18 21 26 66 99 99 99 99\n",
    "24 26 56 99 99 99 99 99\n",
    "47 66 99 99 99 99 99 99\n",
    "99 99 99 99 99 99 99 99\n",
    "99 99 99 99 99 99 99 99\n",
    "99 99 99 99 99 99 99 99\n",
    "99 99 99 99 99 99 99 99\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight our quantization tables with respect to the FDM\n",
    "Since we thresholded all regions through a hard selector, we wish to retain that some areas that fall outside this threshold may still be more important than other areas.\n",
    "To account for these regions, we use the values of the original FDM in those regions to weight our compression rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "luminance_quantization_table = copy.deepcopy(JPEG_STANDARD_LUMINANCE_QUANTIZATION_TABLE)\n",
    "chrominance_quantization_table = copy.deepcopy(JPEG_STANDARD_CHROMINANCE_QUANTIZATION_TABLE)\n",
    "\n",
    "print 'size new luminance quantization table: ' + str(luminance_quantization_table.shape)\n",
    "print 'size new chrominance quantization table: ' + str(chrominance_quantization_table.shape)\n",
    "\n",
    "COMPRESSION_RATE_LUMINANCE = 3 # The aggressiveness of the division during quantization, note that a rate of 1 still causes lossy compression!\n",
    "COMPRESSION_RATE_CHROMINANCE = 1\n",
    "\n",
    "# something with compression_rate*weight\n",
    "luminance_quantization_table *= COMPRESSION_RATE_LUMINANCE\n",
    "chrominance_quantization_table *= COMPRESSION_RATE_CHROMINANCE\n",
    "\n",
    "print luminance_quantization_table\n",
    "print chrominance_quantization_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1\n",
    "quantization_tables = [\n",
    "    luminance_quantization_table,  # Y channel.\n",
    "    chrominance_quantization_table,  # Cr channel.\n",
    "    chrominance_quantization_table,  # Cb channel.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1\n",
    "def quantizemacroblock_dct(macroblock, quantization_table):\n",
    "    \"\"\"\n",
    "    Quantize DCT coefficients of a macroblock given a quantization table.\n",
    "\n",
    "    DCT coefficients are real values.\n",
    "    We first scale them (the larger the coefficients in the quantization table,\n",
    "    the smaller will be the scaled values).\n",
    "    Finally, we round off the scaled values in order to approximate each coefficient\n",
    "    to an integer value (quantization process).\n",
    "\n",
    "    The quantized DCT coefficients will be used with loss-less compression techniques\n",
    "    in order to store them efficiently.\n",
    "    \"\"\"\n",
    "    assert np.shape(macroblock) == np.shape(quantization_table), (\n",
    "      np.shape(macroblock), np.shape(quantization_table))\n",
    "\n",
    "    macroblock_dct = cv2.dct(macroblock)\n",
    "    macroblock_dct = np.round(np.divide(macroblock_dct, quantization_table))\n",
    "\n",
    "    quantized_macroblock_dct = macroblock_dct\n",
    "\n",
    "    return quantized_macroblock_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1 and modified to incorporate regions from our FDMs.\n",
    "# Quantize regions of an image\n",
    "def quantize_image_YCrCb_dct(image_YCrCb_subsampled, scaled_quantization_tables, image_size, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "    _vprint('Quantizing DCT coefficients')\n",
    "\n",
    "    # Initialization and parameters.\n",
    "    image_YCrCb_dct_quantized = []\n",
    "    block_size = MACROBLOCK_SIZE\n",
    "    scaled_fdm = computeSelectionMacroBlock(inverted_fdm/float(np.max(inverted_fdm)))\n",
    "\n",
    "    # Each channel is independently processed.\n",
    "    for channel_index, (channel, quantization_table) in enumerate(\n",
    "        zip(image_YCrCb_subsampled, scaled_quantization_tables)):\n",
    "\n",
    "        # Channel properties.\n",
    "        channel_size = np.shape(channel)\n",
    "        number_of_horizontal_macroblocks, number_of_vertical_macroblocks = (\n",
    "            get_number_of_macroblocks(channel_size, block_size))\n",
    "        quantization_table_data_type = quantization_table.dtype\n",
    "\n",
    "        _vprint('. channel #%d' % channel_index)\n",
    "        _vprint('  channel size: %s' % format_image_size(channel_size))\n",
    "        _vprint('  number of macroblocks: %d x %d' % (\n",
    "            number_of_horizontal_macroblocks, number_of_vertical_macroblocks))\n",
    "\n",
    "        # Initialize quantized channel matrix.\n",
    "        channel_dct_quantized = np.zeros(channel_size, dtype=quantization_table_data_type)\n",
    "\n",
    "        # Our boolean matrix to indicate which macroblocks should be quantized.\n",
    "        macroblock_selector = channelRegions[channel_index]\n",
    "\n",
    "        # Each macroblock is independently quantized.\n",
    "        for row in range(number_of_vertical_macroblocks):\n",
    "            for column in range(number_of_horizontal_macroblocks):\n",
    "                # Extract the macroblock from the channel matrix.\n",
    "                (left_top, right_bottom) = macroblock_coords_to_pixel_vertices((column, row), block_size)\n",
    "                macroblock = channel[left_top[1]:right_bottom[1]+1, left_top[0]:right_bottom[0]+1]\n",
    "\n",
    "                if macroblock_selector[column, row] == 1:\n",
    "                    # Get the FDM scalar\n",
    "                    FDM_scalar = scaled_fdm[column, row]\n",
    "                    # Compute and quantize DCT coefficients.\n",
    "                    macroblock_dct_quantized = quantizemacroblock_dct(\n",
    "                        macroblock.astype(quantization_table_data_type), FDM_scalar*quantization_table)\n",
    "                else:\n",
    "                    macroblock_dct_quantized = macroblock\n",
    "                # Copy macroblock data.\n",
    "                channel_dct_quantized[\n",
    "                    left_top[1]:right_bottom[1]+1, left_top[0]:right_bottom[0]+1] = (\n",
    "                        macroblock_dct_quantized)\n",
    "\n",
    "        _vprint('  quantized DCT coefficients matrix size: %d x %d' % np.shape(channel_dct_quantized))\n",
    "        image_YCrCb_dct_quantized.append(channel_dct_quantized)\n",
    "\n",
    "    return image_YCrCb_dct_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1\n",
    "# We quantize our image and obtain the Discrete Cosine Transform Coefficients.\n",
    "image_YCrCb_dct_quantized = quantize_image_YCrCb_dct(\n",
    "    image_YCrCb_subsampled, quantization_tables, im_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy encoding\n",
    "Now we have quantified our image, we are going to compress it with the Huffman coding.\n",
    "It encodes every symbol which occur in the image to a prefix code based on their frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run-length encoding with zig-zag\n",
    "When we look at our quantized image, we can see that there are many zeros in our every channel. To reduce the number of bits necessary to store our data, we use run-length encoding(RLE).  RLE counts the amount of consecutive characters and stores the number with the corresponding symbol. For JPEG they changed it a bit by only storing the consecutive amount of zeros, without explicitly indicating it is for the zeros. Furthermore they store the non-zero number that occurred after those zeros. JPEG also store the amount of bytes necessary to store the non-zero number, but for our implantation we do not need it, so we omitted it.\n",
    "To optimally find a long chain of zeros, they also zigzag through the image. They zigzag through a macroblock, because of the quantization table, where the least compression happens in the top left corner. When moving further to the bottom right corner, the values are getting compressed more and more zeros are visible.\n",
    "Lastly, because most macroblocks, have only 1 non-zero number which is the top left one, we will only store the value of the first value of the first macroblock. For the blocks after this one, we store the difference between the previous block.\n",
    "So our data will be stored in pairs, like this: amount of zeros, non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Because we only have 8x8 MACROBLOCKS we hardcode the zigzag.\n",
    "j_zig = [0,1,0,0,1,2,3,2,1,0,0,1,2,3,4,5,4,3,2,1,0,0,1,2,3,4,5,6,7,6,5,4,3,2,1,0,1,2,3,4,5,6,7,7,6,5,4,3,2,3,4,5,6,7,7,6,5,4,5,6,7,7,6,7]\n",
    "i_zig = [0,0,1,2,1,0,0,1,2,3,4,3,2,1,0,0,1,2,3,4,5,6,5,4,3,2,1,0,0,1,2,3,4,5,6,7,7,6,5,4,3,2,1,2,3,4,5,6,7,7,6,5,4,3,4,5,6,7,7,6,5,6,7,7]\n",
    "\n",
    "#Zig-zag through a size macroblock by macroblock array and encode it with run-length encoding (RLE)\n",
    "def zigzag_macroblock(quant_arr_macroblock_size, quant_table, count_zeros, DC, is_first):\n",
    "    for zig_all in range(MACROBLOCK_SIZE*MACROBLOCK_SIZE):\n",
    "        quant_arr_value = quant_arr_macroblock_size[i_zig[zig_all]][j_zig[zig_all]]\n",
    "        if zig_all ==  0:\n",
    "            temp_DC_store = quant_arr_value\n",
    "            quant_arr_value = quant_arr_value - DC #Remove '- DC' if you want to remove the calculation between first elements of blocks\n",
    "            if is_first == True:\n",
    "                DC = quant_arr_value\n",
    "            else:\n",
    "                DC = temp_DC_store\n",
    "            is_first = False\n",
    "        if quant_arr_value != 0:\n",
    "            quant_table.append(count_zeros)\n",
    "            quant_table.append(quant_arr_value)\n",
    "            count_zeros = 0\n",
    "        else:\n",
    "            count_zeros += 1\n",
    "    return count_zeros, quant_table, DC, is_first\n",
    "\n",
    "def encode_zigzag(quant_arr):\n",
    "    #print np.asarray(quant_arr)[0].shape\n",
    "    #print quant_arr[0].shape\n",
    "    zigzag_quant = []\n",
    "    trailing_zeros = 0\n",
    "    for channel in range(0,len(quant_arr)):\n",
    "        first_round = True\n",
    "        trailing_zeros = 0\n",
    "        DC_coefficient = 0 #Start DC value\n",
    "        zigzag_quant.append([])\n",
    "        for i in range(0, quant_arr[channel].shape[0], MACROBLOCK_SIZE):\n",
    "            for j in range(0, quant_arr[channel].shape[1], MACROBLOCK_SIZE):\n",
    "                trailing_zeros, zigzag_quant[channel], DC_coefficient, first_round = zigzag_macroblock(quant_arr[channel][i:i+MACROBLOCK_SIZE,j:j+MACROBLOCK_SIZE], zigzag_quant[channel], trailing_zeros, DC_coefficient,first_round)\n",
    "        zigzag_quant[channel].append(0) #End with 2 zeros, so we know there are only trailing zeros left.\n",
    "        zigzag_quant[channel].append(0)\n",
    "    return zigzag_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reference: https://stackoverflow.com/questions/11587044/how-can-i-create-a-tree-for-huffman-encoding-and-decoding\n",
    "\n",
    "# The Huffman tree is stored inside this object.\n",
    "class HuffmanNode(object):\n",
    "    def __init__(self, left=None, right=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def children(self):\n",
    "        return((self.left, self.right))\n",
    "\n",
    "# We build a Huffman tree based on the frequency of each symbol.\n",
    "def create_tree(frequencies):\n",
    "    p = PriorityQueue()\n",
    "    for value in frequencies:    # 1. Create a leaf node for each symbol\n",
    "        p.put(value)             #    and add it to the priority queue\n",
    "    while p.qsize() > 1:         # 2. While there is more than one node\n",
    "        l, r = p.get(), p.get()  # 2a. remove two highest nodes\n",
    "        node = HuffmanNode(l, r) # 2b. create internal node with children\n",
    "        p.put((l[0]+r[0], node)) # 2c. add new node to queue\n",
    "    return p.get()               # 3. tree is complete - return root node\n",
    "\n",
    "# Recursively walk the tree down to the leaves,\n",
    "#   assigning a code value to each symbol\n",
    "def walk_tree(node, prefix=\"\", code={}):\n",
    "    if(isinstance(node[1], HuffmanNode)):\n",
    "        walk_tree(node[1].children()[0],prefix=prefix+'0')\n",
    "        walk_tree(node[1].children()[1],prefix=prefix+'1')\n",
    "    else:\n",
    "        code.update({node[1]:prefix})\n",
    "    return(code)\n",
    "\n",
    "#Create a Huffman table for each array element in 'quant_arr'\n",
    "def huffman_encoding(quant_arr):\n",
    "    huffman_table = []\n",
    "    for i in range(0,len(quant_arr)): #Loop all array elements\n",
    "        huffman_table.append([])\n",
    "        unique, counts = np.unique(quant_arr[i], return_counts=True)\n",
    "        store_freq = list(zip(counts,unique))\n",
    "        node = create_tree(store_freq)\n",
    "        code = walk_tree(node) #Huffman codes\n",
    "        for j in sorted(store_freq, reverse=True): #Sort by frequency\n",
    "            #print(j[1], '{:6.2f}'.format(j[0]), code[j[1]]) #Print the table\n",
    "            huffman_table[i].append((j[1], code[j[1]]))\n",
    "    return huffman_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zigzag_quant = encode_zigzag(image_YCrCb_dct_quantized)\n",
    "huffman_quant_table = huffman_encoding(zigzag_quant)\n",
    "#huffman_quant_table = huffman_encoding(image_YCrCb_dct_quantized)\n",
    "print np.asarray(huffman_quant_table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canonical Huffman codebook: Encoding Huffman code into a Canonical Huffman tree\n",
    "This step transforms our Huffman code we computed over our DCCs into a Canonical Huffman tree. <br />\n",
    "This is done by a few steps, as follows: <br />\n",
    "* First, the bit-length of all the Huffman code w.r.t. its symbol is computed.\n",
    "* Thereafter, the Huffman code is sorted in increasing order based on the **bit-length** of all the symbols.\n",
    "* Finally, the amount of symbols with **equal** bit-length are counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeCanonicalCode(huffman_quant_table, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "\n",
    "    num_channels = len(huffman_quant_table)\n",
    "\n",
    "    canonicalCodebook = []\n",
    "\n",
    "    # We compute the canonical code per channel\n",
    "    for i in range(0, num_channels):\n",
    "        _vprint('**** Begin Huffman code: ' + str(i+1) + ' ****')\n",
    "        _vprint(huffman_quant_table[i])\n",
    "        _vprint('-----------------')\n",
    "\n",
    "        # Sort our code to ascending order for bit-length of symbols\n",
    "        num_codebook_symbols = np.asarray(huffman_quant_table[i]).shape[0]\n",
    "        chm_BitLengthFreq = np.zeros((num_codebook_symbols, 2))\n",
    "        for j in range(0, len(chm_BitLengthFreq)):\n",
    "            chm_BitLengthFreq[j][0] = huffman_quant_table[i][j][0]\n",
    "            chm_BitLengthFreq[j][1] = len(huffman_quant_table[i][j][1])\n",
    "\n",
    "        _vprint('Canonical Huffman code bit length frequency')\n",
    "        _vprint(chm_BitLengthFreq)\n",
    "        _vprint('-----------------')\n",
    "\n",
    "        chm_sortedByBitLength = np.asarray(sorted(chm_BitLengthFreq, key=lambda x: x[1]))\n",
    "\n",
    "        _vprint('Sorted canonical Huffman code by bit length frequency')\n",
    "        _vprint(chm_sortedByBitLength)\n",
    "        _vprint('-----------------')\n",
    "\n",
    "        # Compute bit length sizes, insert a 0 when a bit length size is 'missing'.\n",
    "        max_bit_length = int(max(chm_sortedByBitLength[:, 1]))\n",
    "        bitLengthSorted = np.zeros((max_bit_length, 1), dtype = int)\n",
    "\n",
    "        for i in range(0, len(chm_sortedByBitLength)):\n",
    "            bitLengthSorted[int(chm_sortedByBitLength[i][1])-1] += 1\n",
    "\n",
    "        bitLengthSorted = bitLengthSorted.flatten()\n",
    "        _vprint('Compute bit length sizes')\n",
    "        _vprint(bitLengthSorted)\n",
    "        _vprint('-----------------')\n",
    "\n",
    "        all_symbols = chm_sortedByBitLength[:, 0]\n",
    "        chm_codebook = [bitLengthSorted.T, all_symbols]\n",
    "        canonicalCodebook.append(chm_codebook)\n",
    "        _vprint('Canonical bit length sorted codebook')\n",
    "        _vprint(np.asarray(chm_codebook))\n",
    "        _vprint('-----------------')\n",
    "    return np.asarray(canonicalCodebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "canonicalCodebook = computeCanonicalCode(huffman_quant_table, bln_verbose=False)\n",
    "print canonicalCodebook[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the canonical Huffman codebook\n",
    "We decode the canonical Huffman codebook to derive the encoded canonical Huffman code with its mapping to original symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct canonical Huffman code\n",
    "def decodeCanonicalCodebook(chm_codebook, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "\n",
    "    lookup_table = []\n",
    "    for k in range(0, len(chm_codebook)):\n",
    "        symbols = chm_codebook[k][1]\n",
    "        freq = [int(float(x)) for x in chm_codebook[k][0]]\n",
    "\n",
    "        init_bit_index = 0\n",
    "        # Find the index of the least amount of bits we need for any symbol\n",
    "        for i in range(0, len(freq)):\n",
    "            if freq[i] > 0:\n",
    "                init_bit_index = i+1\n",
    "                break\n",
    "\n",
    "        # Reconstruct our symbol frequencies\n",
    "        bit_length = []\n",
    "        for i in range(0, len(freq)):\n",
    "            if freq[i] > 0: # Code with that bit length is *used* in actual codebook\n",
    "                for j in range(0, freq[i]):\n",
    "                    bit_length.append(i+1)\n",
    "\n",
    "        _vprint('Reconstructed symbol frequencies to use in sequential order')\n",
    "        _vprint(bit_length)\n",
    "        _vprint('-----------------')\n",
    "\n",
    "        _vprint('Decoded codebook -> Canonical Huffman code')\n",
    "        ch_code = [] # Save our huffman code here\n",
    "\n",
    "        # Construct our first code\n",
    "        trailing_bits = 2 + init_bit_index # 2 characters reserved for characters indicating binary format '0b'\n",
    "        code = 0b0 # Starting code in pure binary form (note: that this is without any trailing 0 used for computation)\n",
    "        _vprint((symbols[0], format(0, '#0'+ str(trailing_bits) + 'b')[2:]))\n",
    "\n",
    "        ch_code.append([symbols[0], format(0, '#0'+ str(trailing_bits) + 'b')[2:]]) # Store first code\n",
    "\n",
    "        # Construct the rest of our canonical code (pseudocode from [wikipedia](https://en.wikipedia.org/wiki/Canonical_Huffman_code#Pseudo_code))\n",
    "        for i in range(1, len(symbols)):\n",
    "            bit_length_next = bit_length[i]\n",
    "            current_bit_length = (code + 1).bit_length()\n",
    "            code = (code + 1) << abs(bit_length_next - current_bit_length)\n",
    "            ch_code.append([symbols[i], str(bin(code)[2:])])\n",
    "            _vprint((symbols[i], bin(code)[2:]))\n",
    "\n",
    "        _vprint('-----------------')\n",
    "        _vprint(('Codebook:', ch_code))\n",
    "        lookup_table.append(ch_code)\n",
    "    return np.asarray(lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "canonicalHuffmanCode = decodeCanonicalCodebook(canonicalCodebook, bln_verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the image with Canonical Huffman code\n",
    "We now have our lookup table and a tree to derive the lookup table ready! <br />\n",
    "This means all that remains is to actually encode our DCTCs of the quantizated image. <br />\n",
    "Then all we need to save is the tree (i.e. the codebook) **and** our encoded DCTCs.\n",
    "\n",
    "We put our lookup table in the form of a dictionary to make it more easy to have a direct mapping from our code to symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a numpy array into a dictionary for easy lookup\n",
    "def convertToLookupTable(canonicalHuffmanCodebook):\n",
    "    lookup_table = []\n",
    "    # Go through Y, Cr, Cb\n",
    "    for i in range(0, len(canonicalHuffmanCodebook)):\n",
    "        channel_mapping = {}\n",
    "\n",
    "        # Go through all the symbol:code pairs\n",
    "        for j in range(0, len(canonicalHuffmanCodebook[i])):\n",
    "            symbol = canonicalHuffmanCodebook[i][j][0]\n",
    "            code = canonicalHuffmanCodebook[i][j][1]\n",
    "            channel_mapping[symbol] = code\n",
    "        lookup_table.append(channel_mapping)\n",
    "\n",
    "    return lookup_table\n",
    "\n",
    "lookupTable = convertToLookupTable(canonicalHuffmanCode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the RLE DCTCs\n",
    "We now encode the Run-Length Encoded DCTCs with our lookup table.\n",
    "This should result in a very compact representation of our DCTCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encodes a quantized zig-zag image with Canonical Huffman code\n",
    "def encodeDCTQuantizedZigZag(image_YCrCb_dct_quantized, lookupTable):\n",
    "    num_channels = np.asarray(image_YCrCb_dct_quantized).shape[0]\n",
    "\n",
    "    chm_encoded_dct_quantized = []\n",
    "    for i in range(0, num_channels):\n",
    "        encodedChannel = []\n",
    "        lookupTableChannel = lookupTable[i] # Get the lookup table matching the channel\n",
    "        for j in range(0, len(image_YCrCb_dct_quantized[i])):\n",
    "            DCTCValue = image_YCrCb_dct_quantized[i][j]\n",
    "            encodedDCTC = lookupTableChannel[DCTCValue]\n",
    "            encodedChannel.append(encodedDCTC)\n",
    "        chm_encoded_dct_quantized.append(np.asarray(encodedChannel))\n",
    "    chm_encoded_dct_quantized = np.asarray(chm_encoded_dct_quantized)\n",
    "\n",
    "    return chm_encoded_dct_quantized\n",
    "\n",
    "chm_encoded_dct_quantized = encodeDCTQuantizedZigZag(zigzag_quant, lookupTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to file\n",
    "We are now ready to save the encoded image and its codebook to file.\n",
    "We pack the data by converting all data to hexadecimal representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encodeCodeBook(FILEIN, canonicalCodebook, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "\n",
    "    _vprint('Encoding codebook raw format...')\n",
    "    # Now append the codebook to the bitstream: we write how much numbers there are for the Y and Cr channels\n",
    "    codebook_bitstream = ''\n",
    "    for i in range(0, len(canonicalCodebook)):\n",
    "        codebook = canonicalCodebook[i]\n",
    "        freqs = codebook[0]\n",
    "        symbols = codebook[1]\n",
    "        freq_size = len(freqs)\n",
    "        symbol_size = len(symbols)\n",
    "\n",
    "        # Add the frequencies\n",
    "        for j in range(0, freq_size):\n",
    "            codebook_bitstream += str(int(freqs[j]))\n",
    "            if j < freq_size-1:\n",
    "                codebook_bitstream += ','\n",
    "        codebook_bitstream += '|'\n",
    "\n",
    "        # Add the symbols\n",
    "        for m in range(0, symbol_size):\n",
    "            codebook_bitstream += str(int(symbols[m]))\n",
    "            if m < symbol_size-1:\n",
    "                codebook_bitstream += ','\n",
    "\n",
    "        codebook_bitstream += '|'\n",
    "    _vprint(codebook_bitstream)\n",
    "\n",
    "    f = open(FILEIN, 'w')\n",
    "    f.write(codebook_bitstream)\n",
    "    print len(codebook_bitstream)\n",
    "    f.close()\n",
    "    _vprint('Finished encoding codebook raw format...')\n",
    "\n",
    "def encodeImageRaw(FILEIN, chm_encoded_dct_quantized, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "\n",
    "    _vprint('Encoding bitstream raw format...')\n",
    "    \n",
    "    # Write all content to file as raw output\n",
    "    num_channels = chm_encoded_dct_quantized.shape[0]\n",
    "    f = open(FILEIN, 'a')\n",
    "\n",
    "    for i in range(0, num_channels):\n",
    "        channel_encoded_dct_quantized = chm_encoded_dct_quantized[i].flatten()\n",
    "        channel_encoded_dct_quantized_size = len(channel_encoded_dct_quantized)\n",
    "        for j in range(0, channel_encoded_dct_quantized_size):\n",
    "            code = channel_encoded_dct_quantized[j]\n",
    "            f.write(code)\n",
    "            \n",
    "    f.close()\n",
    "    _vprint('Finished encoding bitstream raw format...')\n",
    "\n",
    "FILEIN = 'encodedimage.raw'\n",
    "\n",
    "encodeCodeBook(FILEIN, canonicalCodebook)\n",
    "encodeImageRaw(FILEIN, chm_encoded_dct_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILEOUT = 'encodedimage.icy'\n",
    "\n",
    "# Writes the entire encoded image to our file\n",
    "def writeIcyImage(FILEOUT, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "\n",
    "    _vprint('Encoding to iCompress YUV format...')\n",
    "    f = open(FILEIN, 'r')\n",
    "\n",
    "    for line in f:\n",
    "        delimiter = '|'\n",
    "        delimitedLine = [e+delimiter for e in line.split(delimiter) if e]\n",
    "        bitstream = delimitedLine[-1][:-1] # Remove a '|' as result of the split\n",
    "    f.close()\n",
    "    bitstream_length = len(bitstream)\n",
    "    BIT_ENCODING_SIZE = 32\n",
    "\n",
    "    f = open(FILEOUT, 'w')\n",
    "    # Write codebook as regular\n",
    "    codebook = np.asarray(delimitedLine[:-1]).flatten()\n",
    "    _vprint('First 32 bits: ' + str(bitstream[0:32]))\n",
    "    for i in range(0, len(codebook)):\n",
    "        f.write(codebook[i])\n",
    "    for i in range(0, bitstream_length, BIT_ENCODING_SIZE):\n",
    "        endIndex = BIT_ENCODING_SIZE\n",
    "        if i + BIT_ENCODING_SIZE > bitstream_length:\n",
    "            endIndex = bitstream_length\n",
    "        bits = bitstream[i:i+endIndex]\n",
    "        int_value = int(bits, base = 2)\n",
    "        bin_array = struct.pack('I', int_value)\n",
    "        f.write(bin_array)\n",
    "    f.close()\n",
    "    _vprint('Successfully encoded raw to iCompress YUV!')\n",
    "writeIcyImage(FILEOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading our encoded image\n",
    "We demonstrate that our encoded image can be easily decoded again to its original encoded bitstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILEIN = 'encodedimage.icy'\n",
    "\n",
    "def readIcyImage(FILEIN):\n",
    "    FILEOUT = FILEIN[:-4] + '.raw'\n",
    "    marker_count = 0\n",
    "    f_in = open(FILEIN, 'r')\n",
    "    f_out = open(FILEOUT, 'w')\n",
    "    bytestream = ''\n",
    "    content = f_in.readlines()\n",
    "\n",
    "    # Read in bytestream\n",
    "    for line in content:\n",
    "        bytestream += line.encode('hex')\n",
    "    print len(bytestream)\n",
    "    f_test = open('stream', 'w')\n",
    "    begin_index = 0\n",
    "    # Convert bytestream to bitstream\n",
    "    while begin_index < len(bytestream): # We iterate over 2 hexes a time, due to our encoder\n",
    "        if begin_index + 8 > len(bytestream):\n",
    "            codes = bytestream[begin_index:].zfill(8)\n",
    "        else:\n",
    "            codes = bytestream[begin_index:begin_index+8].zfill(8)\n",
    "        ascii = codes.decode('hex') # Our codebook is in plain ascii\n",
    "        # Check when to start also unpacking the binary data\n",
    "        if marker_count < 6:\n",
    "            binary = ascii\n",
    "            if '|' in ascii and marker_count == 5:\n",
    "                marker_count += 1\n",
    "                new_end_index = begin_index + 2*ascii.index('|')+2 # we have 4 ascii characters and took 8 characters as hex, index is from 0\n",
    "                binary = bytestream[begin_index:new_end_index].decode('hex')\n",
    "                begin_index = new_end_index\n",
    "            elif '|' in ascii:\n",
    "                marker_count += 1\n",
    "                begin_index += 8\n",
    "            else:\n",
    "                begin_index += 8\n",
    "        else:\n",
    "            orig_int_value = struct.unpack('I', codes.decode('hex'))[0]\n",
    "            binary = bin(orig_int_value)[2:] # Skip '0b' bit marker\n",
    "            if len(binary) != 32:\n",
    "                binary = binary.zfill(32)\n",
    "            begin_index += 8\n",
    "        f_out.write(binary)\n",
    "    f_out.close()\n",
    "    f_in.close()\n",
    "\n",
    "# Execute reading in our .icy image to convert to .raw again\n",
    "readIcyImage(FILEIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping bitstream to the DCTCs\n",
    "We read the bitstream and bit per bit and map the codes to its original DCTC using the Canonical Huffman codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readRaw(FILEIN):\n",
    "    f_in = open(FILEIN, 'r')\n",
    "    for line in f_in:\n",
    "        bitstream = line\n",
    "    f_in.close()\n",
    "\n",
    "    return bitstream\n",
    "\n",
    "def readCodebook(bitstream):\n",
    "    start_index = 0\n",
    "    end_index = start_index + 1\n",
    "    marker_found = 0\n",
    "\n",
    "    print 'Decoding codebook...'\n",
    "    # Decode codebook\n",
    "    while not marker_found == 6:\n",
    "        bits =  bitstream[start_index:end_index]\n",
    "        if '|' in bits:\n",
    "            marker_found += 1\n",
    "        start_index = end_index\n",
    "        end_index += 1\n",
    "\n",
    "    codebook_string = bitstream[0:end_index-1]\n",
    "    codebook = codebook_string.split('|')[:-1]\n",
    "    wrapper = []\n",
    "    for i in range(0, len(codebook), 2):\n",
    "        freqs = np.asarray(codebook[i].split(','))\n",
    "        symbols = np.asarray(codebook[i+1].split(','))\n",
    "        wrapper.append([freqs, symbols])\n",
    "    wrapper = np.asarray(wrapper)\n",
    "\n",
    "    canonicalHuffmanCode_decoded = decodeCanonicalCodebook(wrapper, bln_verbose=False)\n",
    "    lookupTable_decoded = convertToLookupTable(canonicalHuffmanCode_decoded)\n",
    "    inv_lookupTable_decoded = [{v: k for k, v in table.iteritems()} for table in lookupTable_decoded]\n",
    "\n",
    "    return start_index, inv_lookupTable_decoded\n",
    "\n",
    "def decodeImage(bitstream, lookupTable, begin_index):\n",
    "    all_dctcs = []\n",
    "    dctcs = []\n",
    "    prev = 1\n",
    "    dctc = 1\n",
    "    channel_lookupTable = lookupTable[0]\n",
    "    start_index = begin_index\n",
    "    print 'Decoding Y channel...'\n",
    "    while start_index <= bitstream_length:\n",
    "        if int(prev) == 0 and int(dctc) == 0 and len(all_dctcs) == 0:\n",
    "            dctc = 1\n",
    "            channel_lookupTable = lookupTable[1]\n",
    "            all_dctcs.append(dctcs)\n",
    "            dctcs = []\n",
    "            print 'Bit index:', start_index\n",
    "            print 'Decoding Cr channel...'\n",
    "        elif int(prev) == 0 and int(dctc) == 0 and len(all_dctcs) == 1:\n",
    "            dctc = 1\n",
    "            channel_lookupTable = lookupTable[2]\n",
    "            all_dctcs.append(dctcs)\n",
    "            dctcs = []\n",
    "            print 'Bit index:', start_index\n",
    "            print 'Decoding Cb channel...'\n",
    "        prev = dctc\n",
    "\n",
    "        if len(all_dctcs) < 3:\n",
    "            end_index = start_index + 1\n",
    "            max_val = max([len(v) for v in channel_lookupTable.keys()])\n",
    "\n",
    "            while (end_index - start_index) <= max_val:\n",
    "                bitstring = bitstream[start_index:end_index]\n",
    "                if bitstring in channel_lookupTable:\n",
    "                    dctc = channel_lookupTable[bitstring]\n",
    "                    dctc_index_found = end_index\n",
    "                end_index += 1\n",
    "            dctcs.append(dctc)\n",
    "            start_index = dctc_index_found\n",
    "    all_dctcs.append(dctcs)\n",
    "    print 'Done decoding channels'\n",
    "    print 'Bit index:', start_index\n",
    "    return all_dctcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Decode our bitstream to get our zig-zagged DCTCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILEIN = 'encodedimage.raw'\n",
    "bitstream = readRaw(FILEIN)\n",
    "bitstream_length = len(bitstream)\n",
    "print 'Bitstream length:', bitstream_length, 'bits'\n",
    "\n",
    "begin_index, inv_lookupTable_decoded = readCodebook(bitstream)\n",
    "all_dctcs = decodeImage(bitstream, inv_lookupTable_decoded, begin_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoded RLE DCTCs\n",
    "Verify that our decoded RLE DCTCs correspond to the original RLE DCTCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare = lambda x, y: collections.Counter(x) == collections.Counter(y)\n",
    "\n",
    "print 'Decoded Y Zig-zag DCTCs equal to original?', compare(map(int, all_dctcs[0]), map(int, zigzag_quant[0]))\n",
    "print 'Decoded Cr Zig-zag DCTCs equal to original?', compare(map(int, all_dctcs[1]), map(int, zigzag_quant[1]))\n",
    "\n",
    "# This may happen to be false, but this is due to trailing zeros we needed to add for our decoder to cope with it\n",
    "print 'Decoded Cb Zig-zag DCTCs equal to original?', compare(map(int, all_dctcs[2]), map(int, zigzag_quant[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run length decoding and reverse zig-zag\n",
    "Now we have obtained our RLE values with zigzag, we have to reverse the process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_dc_difference(quant_table):\n",
    "    for channel in range(0,len(quant_table)):\n",
    "        h = 0\n",
    "        w = 0\n",
    "        first_el = 0\n",
    "        if channel == 0:\n",
    "            h = IMAGE_HEIGHT\n",
    "            w = IMAGE_WIDTH\n",
    "        else:\n",
    "            h = IMAGE_HEIGHT/2\n",
    "            w = IMAGE_WIDTH/2\n",
    "        for height_first in range(0,h,MACROBLOCK_SIZE):\n",
    "            for width_first in range(0,w,MACROBLOCK_SIZE):\n",
    "                if height_first == 0 and width_first == 0:\n",
    "                    quant_table[channel][height_first][width_first] = quant_table[channel][height_first][width_first]\n",
    "                else:\n",
    "                    quant_table[channel][height_first][width_first] = first_el + quant_table[channel][height_first][width_first]\n",
    "                first_el = quant_table[channel][height_first][width_first]\n",
    "    return quant_table\n",
    "\n",
    "def check_next_pos(index, macroblock_nr,mw,mh, channel):\n",
    "    if(index == MACROBLOCK_SIZE*MACROBLOCK_SIZE - 1):\n",
    "        index = 0\n",
    "        macroblock_nr += 1\n",
    "        mw += 1\n",
    "        if((macroblock_nr % (IMAGE_WIDTH/MACROBLOCK_SIZE) == 0 and channel == 0) or (macroblock_nr % (IMAGE_WIDTH/MACROBLOCK_SIZE/2) == 0 and channel != 0) ):\n",
    "            mw = 0\n",
    "            mh += 1\n",
    "    else:\n",
    "        index += 1\n",
    "    return index, macroblock_nr,mw,mh\n",
    "\n",
    "def decode_zigzag_pair(quant_arr_pair, quant_table, index, macroblock_nr, channel):\n",
    "    if channel == 0 :\n",
    "        macroblock_width = macroblock_nr % (IMAGE_WIDTH/MACROBLOCK_SIZE)\n",
    "        macroblock_height = math.floor(macroblock_nr / (IMAGE_WIDTH/MACROBLOCK_SIZE))\n",
    "    else:\n",
    "        macroblock_width = macroblock_nr % (IMAGE_WIDTH/MACROBLOCK_SIZE/2)\n",
    "        macroblock_height = math.floor(macroblock_nr / (IMAGE_WIDTH/MACROBLOCK_SIZE/2))\n",
    "    for add_zeros in range(int(quant_arr_pair[0])):\n",
    "        quant_table[int(macroblock_height*MACROBLOCK_SIZE + i_zig[index])][int(macroblock_width*MACROBLOCK_SIZE + j_zig[index])] = 0\n",
    "        index,macroblock_nr,macroblock_width,macroblock_height = check_next_pos(index, macroblock_nr, macroblock_width,macroblock_height, channel)\n",
    "    quant_table[int(macroblock_height*MACROBLOCK_SIZE + i_zig[index])][int(macroblock_width*MACROBLOCK_SIZE + j_zig[index])] = quant_arr_pair[1]\n",
    "    index,macroblock_nr,macroblock_width,macroblock_height = check_next_pos(index, macroblock_nr, macroblock_width,macroblock_height, channel)\n",
    "    return quant_table, index, macroblock_nr\n",
    "\n",
    "def decode_zigzag(decode_quant_arr):\n",
    "    normal_quant = []\n",
    "    trailing_zeros = 0\n",
    "    for channel in range(0,len(decode_quant_arr)):\n",
    "        current_index = 0\n",
    "        current_macroblock = 0\n",
    "        if channel == 0:\n",
    "            normal_quant.append(np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH)))\n",
    "        else:\n",
    "            normal_quant.append(np.zeros((IMAGE_HEIGHT/2, IMAGE_WIDTH/2)))\n",
    "        for i in range(0, len(decode_quant_arr[channel]), 2):\n",
    "            normal_quant[channel], current_index, current_macroblock = decode_zigzag_pair(decode_quant_arr[channel][i:i+2],normal_quant[channel], current_index, current_macroblock, channel)\n",
    "    normal_quant = fix_dc_difference(normal_quant) #With DC difference\n",
    "    return normal_quant\n",
    "\n",
    "decoded_zigzag = decode_zigzag(zigzag_quant) # Decode zig-zag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoded DCTCs\n",
    "We once more verify that our decoding of the RLE is performed successfully.\n",
    "We compare the decoded results to the original DCTCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verify that our decoded DCTCs are completely equal to the DCTCs resulting from the quantization of the image.\n",
    "compare = lambda x, y: collections.Counter(x) == collections.Counter(y)\n",
    "\n",
    "print 'Decoded Y DCTCs equal to original?', compare(map(int, list(decoded_zigzag[0].flatten())), map(int, list(image_YCrCb_dct_quantized[0].flatten())))\n",
    "print 'Decoded Cr DCTCs equal to original?', compare(map(int, list(decoded_zigzag[1].flatten())), map(int, list(image_YCrCb_dct_quantized[1].flatten())))\n",
    "print 'Decoded Cb DCTCs equal to original?', compare(map(int, list(decoded_zigzag[2].flatten())), map(int, list(image_YCrCb_dct_quantized[2].flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1.\n",
    "def decode_quantized_dct_block(macroblock_dct_quantized, quantization_table):\n",
    "    \"\"\"\n",
    "    Decode quantized DCT coefficients given the quantization table used at the encoding step.\n",
    "\n",
    "    This function is dual to quantizemacroblock_dct().\n",
    "    DCT coefficients can be negative, we need to map the macroblock values in the range [0, 255].\n",
    "    \"\"\"\n",
    "    macroblock_dct_quantized = np.multiply(macroblock_dct_quantized, quantization_table)\n",
    "\n",
    "    macroblock_decoded = cv2.idct(macroblock_dct_quantized)\n",
    "    return np.clip(macroblock_decoded, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from the MMSR systems lab 1 with several modifications.\n",
    "def decode_image_YCrCb_dct_quantized(image_YCrCb_dct_quantized, scaled_quantization_tables,\n",
    "                                     image_size, block_size = MACROBLOCK_SIZE, bln_verbose = True):\n",
    "    if bln_verbose:\n",
    "        def _vprint(what):\n",
    "            print what\n",
    "    else:\n",
    "        def _vprint(what):\n",
    "            pass\n",
    "    _vprint('Decoding quantized DCT')\n",
    "    pass\n",
    "\n",
    "    image_height = image_size[0]\n",
    "    image_width = image_size[1]\n",
    "    number_of_channels = image_size[2]\n",
    "    _vprint(number_of_channels)\n",
    "\n",
    "    image_YCrCb_decoded = np.zeros(image_size, dtype=np.uint8)\n",
    "    _vprint('. decoded image size: %d x %d' % (image_width, image_height))\n",
    "\n",
    "    scaled_fdm = computeSelectionMacroBlock(inverted_fdm/float(np.max(inverted_fdm)))\n",
    "\n",
    "    for channel_index, (channel_dct_quantized, quantization_table) in enumerate(zip(image_YCrCb_dct_quantized, scaled_quantization_tables)):\n",
    "        # Channel properties.\n",
    "        channel_size = np.shape(channel_dct_quantized)\n",
    "        number_of_horizontal_macroblocks, number_of_vertical_macroblocks = (get_number_of_macroblocks(channel_size, block_size))\n",
    "\n",
    "        _vprint('. channel #%d' % channel_index)\n",
    "        _vprint('  channel size: %s' % format_image_size(channel_size))\n",
    "        _vprint('  number of macroblocks: %d x %d' % (number_of_horizontal_macroblocks, number_of_vertical_macroblocks))\n",
    "\n",
    "        # Initialize the decoded channel matrix.\n",
    "        decoded_channel = np.zeros(channel_size, dtype=channel_dct_quantized.dtype)\n",
    "\n",
    "        # Our boolean matrix to indicate which macroblocks should be quantized.\n",
    "        macroblock_selector = channelRegions[channel_index]\n",
    "\n",
    "        # Each macroblock is independently decoded.\n",
    "        for row in range(number_of_vertical_macroblocks):\n",
    "            for column in range(number_of_horizontal_macroblocks):\n",
    "                # Extract the macroblock from the quantized DCT coefficients matrix.\n",
    "                (left_top, right_bottom) = macroblock_coords_to_pixel_vertices((column, row), block_size)\n",
    "                macroblock_dct_quantized = channel_dct_quantized[\n",
    "                    left_top[1]:right_bottom[1]+1, left_top[0]:right_bottom[0]+1]\n",
    "\n",
    "                if macroblock_selector[column, row] == 1:\n",
    "                    # Get the FDM scalar\n",
    "                    FDM_scalar = scaled_fdm[column, row]\n",
    "\n",
    "                    # Decode quantized DCT coefficients.\n",
    "                    macroblock_decoded = decode_quantized_dct_block(\n",
    "                        macroblock_dct_quantized, FDM_scalar*quantization_table)\n",
    "                else:\n",
    "                    macroblock_decoded = macroblock_dct_quantized\n",
    "\n",
    "                # Copy macroblock decoded data.\n",
    "                decoded_channel[\n",
    "                    left_top[1]:right_bottom[1]+1, left_top[0]:right_bottom[0]+1] = macroblock_decoded\n",
    "\n",
    "        # Resize the decoded channel matrix.\n",
    "        image_YCrCb_decoded[:, :, channel_index] = cv2.resize(decoded_channel, (image_width, image_height))\n",
    "\n",
    "    return image_YCrCb_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding of the quantized DCTCs of the <code>.icy</code> image\n",
    "We have now successfully decoded our <code>.icy</code> file and reconstructed our original DCTCs. This means we are ready to decode the quantized image. <br />\n",
    "Moreover, we can now also see what our compressed image looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, convertYUV to RGB and compare the original and the compressed image.\n",
    "image_YCrCb_decoded = decode_image_YCrCb_dct_quantized(\n",
    "    decoded_zigzag, quantization_tables, im_size)\n",
    "image_BGR_decoded = cv2.cvtColor(image_YCrCb_decoded, cv2.cv.CV_YCrCb2BGR)\n",
    "\n",
    "ipynb_show_cv2_image(im, 'original')\n",
    "ipynb_show_cv2_image(image_BGR_decoded, '.icy compressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing fixation density maps from eye fixation data\n",
    "This section will handle the computation of fixation density maps (FDMs) from the eye fixation data. The eye fixation data was retrieved from the following location: http://ivc.univ-nantes.fr/en/databases/HD_UHD_Eyetracking_Videos/.\n",
    "\n",
    "An FDM will be computed for each frame seperately. The functions for doing this are listed first, after which the code to execute them is listed.\n",
    "\n",
    "First load the raw fixation data and subsample it to enable more efficient processing. The FDMs will be supersampled back to the video frame size at the end. This does not cause a large loss of quality, because the FDMs don't contain much detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: file_name to read fixations from as a string\n",
    "# Output: List of tuples, with as first element tuple of start and end time of fixation,\n",
    "# and as second element tuple of x and y position of fixation.\n",
    "def read_fixations(file_name):\n",
    "    fixations = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        csv_reader = csv.reader(f, 'excel-tab')\n",
    "        next(csv_reader)\n",
    "        for fixation in csv_reader:\n",
    "            fixations.append(((float(fixation[0]), float(fixation[1])),(int(float(fixation[2])), int(float(fixation[3])))))\n",
    "    return fixations\n",
    "\n",
    "# Input: raw fixations and a subsampling ratio\n",
    "# Output: A subsampled set of fixations\n",
    "def subsample_fixations(fixations, ratio):\n",
    "    fixations_subsampled = []\n",
    "    for fixation in fixations:\n",
    "        fixations_subsampled.append((fixation[0], (fixation[1][0] / ratio, fixation[1][1] / ratio)))\n",
    "\n",
    "    return fixations_subsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the time-based fixations to frame-based fixations. This is necessary to find out which frames a fixation belongs to. `time_fixations_to_frame_fixations()` handles a list of fixations, `time_fixation_to_frame_fixations()` handles one fixation at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: time and frames per second for the video\n",
    "# Ouput: zero-based frame number corresponding to the given time\n",
    "def time_to_frame_number(time, fps):\n",
    "    return max(int(time / 1000.0 * fps), 0)\n",
    "\n",
    "# Input: one fixationn occurrence with the time it occurred at\n",
    "# Output: the input fixation for each frame it occurred in\n",
    "def time_fixation_to_frame_fixations(time_fixation, fps):\n",
    "    frame_fixations = []\n",
    "    fixation_time = time_fixation[0]\n",
    "    start_frame = time_to_frame_number(fixation_time[0], fps)\n",
    "    end_frame = time_to_frame_number(fixation_time[1], fps)\n",
    "    for frame_number in range(start_frame, end_frame + 1):\n",
    "        frame_fixations.append((frame_number,time_fixation[1]))\n",
    "\n",
    "    return frame_fixations\n",
    "\n",
    "# Input: list of fixations indicated by time they occurred.\n",
    "# Output: list of fixations indicated by frame they occurred in.\n",
    "def time_fixations_to_frame_fixations(time_fixations, fps):\n",
    "    frame_fixations = []\n",
    "    for fixation in time_fixations:\n",
    "        frame_fixations.extend(time_fixation_to_frame_fixations(fixation, fps))\n",
    "    return frame_fixations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, organize the fixations in a dictionary to be able to access them efficiently per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: list of fixations indicated by frame number\n",
    "# Output: ditionary with as key frame number, as value list of fixations\n",
    "def fixation_list_to_dict(fixation_list):\n",
    "    fixation_dict = {}\n",
    "    for fixation in fixation_list:\n",
    "        if fixation[0] not in fixation_dict:\n",
    "            fixation_dict[fixation[0]] = []\n",
    "    for fixation in fixation_list:\n",
    "        fixation_dict[fixation[0]].append(fixation[1])\n",
    "    return fixation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each frame, compute the density map using a gaussian function around the original fixation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes gaussian value in 2d space using the given parameters.\n",
    "def gaussian_2d(x, y, x_base, y_base, amplitude = 1, spread = 1):\n",
    "    x_part = float(((x-x_base)**2))/float((2*spread**2))\n",
    "    y_part = float(((y-y_base)**2))/float((2*spread**2))\n",
    "    return amplitude * np.exp(-(x_part + y_part))\n",
    "\n",
    "\n",
    "# Input: List of fixations for one frame\n",
    "# Output: Fixation density map for one frame\n",
    "def fixations_to_fdm(fixations, frame_size, fixation_spread=0.1):\n",
    "    fdm = np.zeros(frame_size)\n",
    "    average_frame_size = (frame_size[0] + frame_size[1]) / 2.0\n",
    "    for fixation in fixations:\n",
    "        for i in range(0, frame_size[0]):\n",
    "            for j in range(0, frame_size[1]):\n",
    "                fdm[i][j] += gaussian_2d(j, i, fixation[0], fixation[1], 1, 0.05 * average_frame_size)\n",
    "    return fdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize each FDM and convert it to uint8 to prepare it for compression by OpenCV to save space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fdm_max_value(fdm):\n",
    "    max_value = 0\n",
    "    for i in range(fdm.shape[0]):\n",
    "        max_value = max(max_value, max(fdm[i]))\n",
    "    return max_value\n",
    "\n",
    "# Input: fdm that has not been normalized\n",
    "# Output: fdm that has been normalized to values between 0 and 1.\n",
    "def normalize_fdm(fdm):\n",
    "    max_value = fdm_max_value(fdm)\n",
    "    normalized_fdm = np.zeros(fdm.shape)\n",
    "    for i in range(fdm.shape[0]):\n",
    "        for j in range(fdm.shape[1]):\n",
    "            normalized_fdm[i][j] = fdm[i][j] / max_value\n",
    "    return np.array(normalized_fdm * 255, dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now supersample each FDM back to the original frame size, and encode it to PNG to save space. Add a decode function too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input: original fdm and frame_size to sample to\n",
    "# Output: supersampled fdm\n",
    "def supersample_fdm(fdm, frame_size):\n",
    "    return cv2.resize(fdm, frame_size, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "# Input: fdm\n",
    "# Output: fdm encoded using png\n",
    "def encode_fdm(fdm):\n",
    "    return cv2.imencode('.png', fdm, (cv2.IMWRITE_PNG_COMPRESSION, 9))[1]\n",
    "\n",
    "# Input: fdm encoded by encode_fdm()\n",
    "# Output: decoded fdm\n",
    "def decode_fdm(fdm):\n",
    "    return np.array(cv2.imdecode(fdm, cv2.CV_LOAD_IMAGE_GRAYSCALE) / 255.0, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally add a function that uses all previous parts of computing the FDMs to implement the whole pipeline. Also add a function to show the final FDMs as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_fdms_all_frames(file_name, fps, frame_size, subsample_ratio):\n",
    "    print('Processing raw data...')\n",
    "    raw_fixations = read_fixations(file_name)\n",
    "    subsampled_fixations = subsample_fixations(raw_fixations, subsample_ratio)\n",
    "    frame_fixations = time_fixations_to_frame_fixations(subsampled_fixations, fps)\n",
    "    fixation_dict = fixation_list_to_dict(frame_fixations)\n",
    "    subsampled_frame_size = (frame_size[0] / subsample_ratio + 1, frame_size[1] / subsample_ratio + 1)\n",
    "\n",
    "    final_fdms = {}\n",
    "    counter = 0\n",
    "    for fixations in fixation_dict:\n",
    "        print('Computing fdm for frame ' + str(counter) + '...')\n",
    "        counter += 1\n",
    "        fdm = fixations_to_fdm(fixation_dict[fixations], subsampled_frame_size, 0.05)\n",
    "        normalized_fdm = normalize_fdm(fdm)\n",
    "        supersampled_fdm = supersample_fdm(normalized_fdm, (FRAME_SIZE[1], FRAME_SIZE[0]))\n",
    "        final_fdms[fixations] = encode_fdm(supersampled_fdm)\n",
    "    return final_fdms\n",
    "\n",
    "def show_fdms(fdms):\n",
    "    fdm_keys = sorted(fdms.iterkeys())\n",
    "    for fdm in fdm_keys:\n",
    "        ipynb_show_image(decode_fdm(fdms[fdm]), 'Frame ' + str(fdm), _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "\n",
    "# Only shows a few examples to save time\n",
    "def show_fdms_examples(fdms):\n",
    "    ipynb_show_image(decode_fdm(fdms[0]), 'Frame 0', _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "    ipynb_show_image(decode_fdm(fdms[50]), 'Frame 50', _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "    ipynb_show_image(decode_fdm(fdms[100]), 'Frame 100', _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "    ipynb_show_image(decode_fdm(fdms[150]), 'Frame 150', _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "    ipynb_show_image(decode_fdm(fdms[200]), 'Frame 200', _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "    ipynb_show_image(decode_fdm(fdms[250]), 'Frame 250', _DEFAULT_IMAGE_FIGSIZE, True)\n",
    "    ipynb_show_image(decode_fdm(fdms[297]), 'Frame 297', _DEFAULT_IMAGE_FIGSIZE, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the `compute_fdms_all_frames()` function on an example video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FPS = 25\n",
    "FRAME_SIZE = (2160, 3840)\n",
    "SUBSAMPLE_RATIO = 288\n",
    "FILE_NAME = \"fixations/BBB_seq1_s3840x2160p25n300v0_Fixations.csv\"\n",
    "\n",
    "fdms = compute_fdms_all_frames(FILE_NAME, FPS, FRAME_SIZE, SUBSAMPLE_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, show the result as an image. Only some examples are shown. To show the FDM for every frame, use the `show_fdms()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_fdms_examples(fdms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress a video\n",
    "Now we have all the FDMs generated for our video, we will start testing our compression method on the video.\n",
    "These helper functions below will help us convert the .YUV file format into an easier iterable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From a YUV444 format, return the Y, U, V as separate components in YUV420 format.\n",
    "def YUV444toYUV422(image_YCrCb_decoded, image_height,image_width):\n",
    "    store_Y_decoded_image = np.chararray(image_height * image_width)\n",
    "    store_Cr_decoded_image = np.chararray(image_height * image_width / 4)\n",
    "    store_Cb_decoded_image = np.chararray(image_height * image_width / 4)\n",
    "    print image_YCrCb_decoded.shape\n",
    "    for i in range(0,image_YCrCb_decoded.shape[0]):\n",
    "        for j in range(0,image_YCrCb_decoded.shape[1]):\n",
    "            store_Y_decoded_image[i*image_width+j] = chr(image_YCrCb_decoded[i][j][0])\n",
    "            if(i % 2 == 0 and j % 2 ==0 ):\n",
    "                store_Cr_decoded_image[i/2*image_width/2+j/2] = chr(image_YCrCb_decoded[i][j][1])\n",
    "                store_Cb_decoded_image[i/2*image_width/2+j/2] = chr(image_YCrCb_decoded[i][j][2])\n",
    "    return store_Y_decoded_image, store_Cr_decoded_image, store_Cb_decoded_image\n",
    "\n",
    "# Play a YUV420 video\n",
    "def YUV2video(filename):\n",
    "    ffmpeg_bin = 'ffmpeg'\n",
    "    command = [ffmpeg_bin,\n",
    "               '-f', 'rawvideo',\n",
    "               '-r',str(FPS),\n",
    "               '-video_size', str(FRAME_SIZE[1])+'x'+str(FRAME_SIZE[0]),\n",
    "               '-pix_fmt', 'yuv420p',\n",
    "               '-i', filename,\n",
    "               '-y',\n",
    "               '-c:v', 'libx264', #H264 encoding\n",
    "               '-preset', 'ultrafast', '-qp', '0', './ultraeye/output.mp4' #Output\n",
    "              ]\n",
    "\n",
    "    try:\n",
    "        sp.check_output(command,stderr=sp.STDOUT)\n",
    "    except sp.CalledProcessError as e:\n",
    "        raise RuntimeError(\"command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting every frame\n",
    "Here we loop over all the frames and one by one will run all the steps for compressing and decompressing our frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FILE_NAME_VIDEO = 'video/BBB_seq1_s3840x2160p25n300v0.yuv'\n",
    "\n",
    "# from shutil import copyfile\n",
    "# FILE_NAME_VIDEO_COPY = 'ultraeye/BBB_seq1_s3840x2160p25n300v0_copy.yuv'\n",
    "# copyfile(FILE_NAME_VIDEO, FILE_NAME_VIDEO_COPY)\n",
    "# FILE_NAME_VIDEO = FILE_NAME_VIDEO_COPY\n",
    "\n",
    "MAX_PIX_VAL = 255\n",
    "\n",
    "# Reference for reading input: https://raspberrypi.stackexchange.com/questions/28033/reading-frames-of-uncompressed-yuv-video-file\n",
    "with open(FILE_NAME_VIDEO, 'rb+') as stream:\n",
    "    Framecount = 0 #The frame it is currently processing\n",
    "    while Framecount != 300: # Amount of frames which will be compressed\n",
    "        print 'Processing frame %d...' % (Framecount)\n",
    "        fwidth = FRAME_SIZE[1]\n",
    "        fheight = FRAME_SIZE[0]\n",
    "        stream.seek(Framecount * fheight * fwidth * 1.5) # Step size of one frame (YUV420 format)\n",
    "        Y = np.fromfile(stream, dtype=np.uint8, count=fwidth*fheight).\\\n",
    "                reshape((fheight, fwidth))\n",
    "        # Load the UV (chrominance) data from the stream, and double its size\n",
    "        U = np.fromfile(stream, dtype=np.uint8, count=(fwidth//2)*(fheight//2)).\\\n",
    "                reshape((fheight//2, fwidth//2)).\\\n",
    "                repeat(2, axis=0).repeat(2, axis=1)\n",
    "        V = np.fromfile(stream, dtype=np.uint8, count=(fwidth//2)*(fheight//2)).\\\n",
    "                reshape((fheight//2, fwidth//2)).\\\n",
    "                repeat(2, axis=0).repeat(2, axis=1)\n",
    "        # Stack the YUV channels together, crop the actual resolution, convert to\n",
    "        # floating point for later calculations, and apply the standard biases\n",
    "        YUV = np.dstack((Y, U, V))[:fheight, :fwidth, :].astype(np.uint8)\n",
    "        im = YUV\n",
    "        im = cv2.cvtColor(im, cv2.cv.CV_YCrCb2BGR)\n",
    "        im_size = np.shape(im) #don't think this ever changes\n",
    "        ##Load new density map here\n",
    "        im_fdm = (decode_fdm(fdms[Framecount]) * 255).astype(np.uint8)\n",
    "        white_threshold = THRESHOLD_SCALAR*MAX_PIX_VAL\n",
    "        inverted_fdm = cv2.bitwise_not(im_fdm)\n",
    "        _, im_fdm_thresh = cv2.threshold(im_fdm, white_threshold, MAX_PIX_VAL, cv2.THRESH_BINARY)\n",
    "        inverted_fdm_thres = cv2.bitwise_not(im_fdm_thresh)\n",
    "        channelRegions = computeChannelRegions(inverted_fdm_thres)\n",
    "\n",
    "\n",
    "        ## Encode image\n",
    "        image_YCrCb_subsampled = subsample_chrominance_channels(im)\n",
    "        image_YCrCb_dct_quantized = quantize_image_YCrCb_dct(\n",
    "        image_YCrCb_subsampled, quantization_tables, im_size)\n",
    "\n",
    "        # Decoding image\n",
    "        image_YCrCb_decoded = decode_image_YCrCb_dct_quantized(\n",
    "            image_YCrCb_dct_quantized, quantization_tables, im_size)\n",
    "\n",
    "        #Write back to file\n",
    "        y_dec, u_dec, v_dec = YUV444toYUV422(image_YCrCb_decoded, FRAME_SIZE[0],FRAME_SIZE[1])\n",
    "        stream.seek(Framecount * fheight * fwidth * 1.5) # For Move back.\n",
    "        stream.write(y_dec) #Store as YUV4:2:0\n",
    "        stream.write(u_dec)\n",
    "        stream.write(v_dec)\n",
    "        Framecount += 1\n",
    "YUV2video(FILE_NAME_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output frame in video\n",
    "Helper function to check if frame in video seems fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ffmpeg_bin = 'ffmpeg'\n",
    "command = [ffmpeg_bin,\n",
    "           '-f', 'rawvideo',\n",
    "           '-r','300',\n",
    "           '-video_size', '3840x2160',\n",
    "           '-pix_fmt', 'yuv420p',\n",
    "           '-i', FILE_NAME_VIDEO,\n",
    "           '-vf', 'select=gte(n\\,2)', #Frame\n",
    "           #'-qscale', '0', #No reduction\n",
    "           '-frames:v', '1','-y',  #Amount of frame\n",
    "           '-f', 'image2pipe',\n",
    "           '-pix_fmt', 'yuv420p',\n",
    "           '-c:v', 'rawvideo',\n",
    "           '-'\n",
    "          ]\n",
    "proc = sp.Popen(command, stdout=sp.PIPE, stderr=sp.PIPE)\n",
    "output, errors = proc.communicate()\n",
    "print output[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "For the evaluation, we will compare the performance of our algorithm against the JPEG algorithm. We will compare the algorithms based on these two properties:\n",
    "- Quality of image after compression, compared to the base image. This will be done using the structural similarity measure.\n",
    "- Compression ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of image\n",
    "For comparing the quality of the final image, we will use the SSIM measure. The SSIM measure is the structural similarity measure that measures the similarity of two images based on how a human would compare them. Applying it to two images results in a value from 0 to 1, where a higher value indicates more similarity.\n",
    "\n",
    "We will compare both the iCompress and JPEG version of the image to the original image using the SSIM measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssim_icompress = skimage.measure.compare_ssim(im, image_BGR_decoded, multichannel=True, gaussian_weights=True)\n",
    "print \"SSIM iCompress = %.6f\" % (ssim_icompress)\n",
    "\n",
    "im_jpeg_encoded = cv2.imencode('.jpg', im, (cv2.IMWRITE_JPEG_QUALITY, 24))[1]\n",
    "im_jpeg_decoded = cv2.imdecode(im_jpeg_encoded, cv2.CV_LOAD_IMAGE_COLOR)\n",
    "\n",
    "ssim_jpeg = skimage.measure.compare_ssim(im, im_jpeg_decoded, multichannel=True, gaussian_weights=True)\n",
    "print \"SSIM JPEG = %.6f\" % (ssim_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the output, the overall quality of the iCompress algorithm with the current settings is about the same as the JPEG algorithm at quality level 24.\n",
    "\n",
    "Let us now visualize the differences by showing the original, iCrompress compressed, and JPEG compressed images side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ipynb_show_cv2_image(im, 'original')\n",
    "ipynb_show_cv2_image(image_BGR_decoded, 'iCompress')\n",
    "ipynb_show_cv2_image(im_jpeg_decoded, 'JPEG quality 24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between iCompress and JPEG is most apparent in the sky: it is of much lower quality in the iCompress case.\n",
    "\n",
    "Let us now zoom in to an area that has been left mostly untouched by iCompress because of the povided fixation density map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertical_slice = slice(750, 2250)\n",
    "horizontal_slice = slice(1750, 2750)\n",
    "ipynb_show_cv2_image(im[vertical_slice, horizontal_slice, :], 'original zoomed')\n",
    "ipynb_show_cv2_image(image_BGR_decoded[vertical_slice, horizontal_slice, :], 'iCompress zoomed')\n",
    "ipynb_show_cv2_image(im_jpeg_decoded[vertical_slice, horizontal_slice, :], 'JPEG quality 24 zoomed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above images, one can see that colored houses on the left of the street and the church are of higher quality in the iCompress case than in the JPEG case. In fact, the iCompress case is similar in quality as expected to the original. The artifacts in the clouds in the iCompress image are even more apparent at this zoom level, though.\n",
    "\n",
    "Let us now zoom in more closely to the colored houses to see the difference more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertical_slice = slice(1250, 1750)\n",
    "horizontal_slice = slice(1750, 2350)\n",
    "ipynb_show_cv2_image(im[vertical_slice, horizontal_slice, :], 'original zoomed even more')\n",
    "ipynb_show_cv2_image(image_BGR_decoded[vertical_slice, horizontal_slice, :], 'iCompress zoomed even more')\n",
    "ipynb_show_cv2_image(im_jpeg_decoded[vertical_slice, horizontal_slice, :], 'JPEG quality 24 zoomed even more')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the houses are clearly of better quality in the iCompress image.\n",
    "\n",
    "### Compression ratio\n",
    "\n",
    "Now we are going to look at the difference in compression ratio between iCompress and JPEG. To do this, we have to retrieve the actual file size of the files on disk that have been compressed by the two algrorihms.\n",
    "\n",
    "The iCompress file has already been written to disk earlier in the notebook. Let's now also write a JPEG file to disk and retrieve it's size. We also retrieve the size of the iCompress file.\n",
    "\n",
    "First we set the JPEG quality to the level at which the quality of iCompress and JPEG is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bytes_to_MB(bytes):\n",
    "    return bytes/float(1024**2)\n",
    "\n",
    "print 'Size of original file: %.3fMB' % bytes_to_MB(os.path.getsize('ultraeye/C01_UHD.tif'))\n",
    "print 'Size of iCompress file: %.3fMB' % bytes_to_MB(os.path.getsize('encodedimage.icy'))\n",
    "\n",
    "cv2.imwrite('im_jpeg_encoded.jpg', im, (cv2.IMWRITE_JPEG_QUALITY, 24))\n",
    "print 'Size of JPEG file: %.3fMB' % bytes_to_MB(os.path.getsize('im_jpeg_encoded.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is quite clear that the JPEG algorithm's compression ratio is much better than iCompress's; actually it is an order of magnitude better. This is probably due to JPEG having much more efficient coding for storage. Though note that the iCompress file is still also an order of magnitude smaller than the original tif file.\n",
    "\n",
    "To be more precise, for this case the compression ratios are as follows:\n",
    "- iCompress: 0.118\n",
    "- JPEG: 0.011\n",
    "\n",
    "Now let's see what happens to the image quality differences when we set the JPEG quality so that the file sizes are about the same. We will use the same street part of the picture as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Size of iCompress file: %.3fMB' % bytes_to_MB(os.path.getsize('encodedimage.icy'))\n",
    "\n",
    "cv2.imwrite('im_jpeg_encoded.jpg', im, (cv2.IMWRITE_JPEG_QUALITY, 98))\n",
    "print 'Size of JPEG file: %.3fMB' % bytes_to_MB(os.path.getsize('im_jpeg_encoded.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"SSIM iCompress = %.6f\" % (ssim_icompress)\n",
    "\n",
    "im_jpeg_encoded_high_quality = cv2.imencode('.jpg', im, (cv2.IMWRITE_JPEG_QUALITY, 98))[1]\n",
    "im_jpeg_decoded_high_quality = cv2.imdecode(im_jpeg_encoded_high_quality, cv2.CV_LOAD_IMAGE_COLOR)\n",
    "\n",
    "ssim_jpeg_high_quality = skimage.measure.compare_ssim(im, im_jpeg_decoded_high_quality, multichannel=True, gaussian_weights=True)\n",
    "print \"SSIM JPEG = %.6f\" % (ssim_jpeg_high_quality)\n",
    "\n",
    "vertical_slice = slice(1250, 1750)\n",
    "horizontal_slice = slice(1750, 2350)\n",
    "ipynb_show_cv2_image(im[vertical_slice, horizontal_slice, :], 'original zoomed even more')\n",
    "ipynb_show_cv2_image(image_BGR_decoded[vertical_slice, horizontal_slice, :], 'iCompress zoomed even more')\n",
    "ipynb_show_cv2_image(im_jpeg_decoded_high_quality[vertical_slice, horizontal_slice, :], 'JPEG quality 24 zoomed even more')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now almost no differences can be seen in quality for the colored buildings on the left side of the street when comparing iCompress and JPEG. However, in the parts where iCompress is not of high quality, JPEG is clearly better.\n",
    "\n",
    "We could try zooming in close to the pixel level to see if the high quality parts of the iCompress image are still better than the JPEG's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertical_slice = slice(1475, 1725)\n",
    "horizontal_slice = slice(1950, 2150)\n",
    "ipynb_show_cv2_image(im[vertical_slice, horizontal_slice, :], 'original extreme zoom')\n",
    "ipynb_show_cv2_image(image_BGR_decoded[vertical_slice, horizontal_slice, :], 'iCompress extreme zoom')\n",
    "ipynb_show_cv2_image(im_jpeg_decoded_high_quality[vertical_slice, horizontal_slice, :], 'JPEG quality 24 extreme zoom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no difference to be seen here, even not between the original and the two compressed versions.\n",
    "\n",
    "## Quality and compression ratio summary\n",
    "In summary, we have seen that the iCompress compression algorithm can deliver better quality than JPEG in its high quality areas when the overall quality level is the same between the two algorithms. This goes at the expense, and quite clearly so, of the lower quality areas, which can suffer quite heavily.\n",
    "\n",
    "The JPEG algorithm is clearly better at getting good quality with a small file size. The files of iCompress are a lot bigger for the same quality. If one chooses the file size to be about the same, the JPEG image will outperform the iCompress image strongly in the low-quality areas of the iCompress image. The quality is about the same in high-quality areas of the iCompress image, in that case."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
